# =============================================================================
# Multi-stage Dockerfile for FastAPI GenAI Service (OPTIMIZED)
# Target size: ~1.8GB (down from 5GB - uses CPU-only PyTorch)
# Memory: 4GB recommended (for Gemini API calls + embeddings)
# CPU: 2 vCPU
# Timeout: 300s (Cloud Run default for services)
# 
# Optimizations:
# - Uses requirements-api.txt (excludes training-only deps: xgboost, lightgbm)
# - CPU-only PyTorch (saves ~3GB of CUDA libraries)
# - Aggressive cache cleanup
# - Optimized layer ordering for better caching
# =============================================================================

# =============================================================================
# Stage 1: Builder - Install dependencies and create wheels
# =============================================================================
FROM python:3.13-slim-bookworm AS builder

# Set working directory
WORKDIR /build

# Install build dependencies (minimal set for compilation)
RUN apt-get update && apt-get install -y --no-install-recommends \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

# Copy API-specific requirements file
COPY requirements-api.txt .

# Create wheels for all dependencies (faster installation in runtime stage)
# Use CPU-only PyTorch index to avoid downloading CUDA libraries
RUN pip install --no-cache-dir --upgrade pip wheel && \
    pip wheel --no-cache-dir --wheel-dir /wheels -r requirements-api.txt

# =============================================================================
# Stage 2: Runtime - Minimal production image
# =============================================================================
FROM python:3.13-slim-bookworm AS runtime

# Set labels for metadata
LABEL maintainer="SG Job Market Team"
LABEL description="FastAPI GenAI service (optimized, CPU-only)"
LABEL version="1.1"

# Set environment variables
ENV PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    PIP_NO_CACHE_DIR=1 \
    PIP_DISABLE_PIP_VERSION_CHECK=1 \
    HOME=/app \
    XDG_CACHE_HOME=/app/.cache \
    HF_HOME=/app/.cache/huggingface \
    TRANSFORMERS_CACHE=/app/.cache/huggingface/transformers \
    SENTENCE_TRANSFORMERS_HOME=/app/.cache/sentence-transformers \
    PORT=8080

# Create app directory and cache directories
WORKDIR /app
RUN mkdir -p /app/.cache/huggingface/transformers /app/.cache/sentence-transformers

# Copy wheels from builder stage
COPY --from=builder /wheels /wheels

# Install dependencies from wheels and clean up aggressively
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir --no-index --find-links=/wheels /wheels/* && \
    rm -rf /wheels /root/.cache /tmp/*

# Copy application code
COPY utils/ /app/utils/
COPY genai/ /app/genai/
COPY nlp/ /app/nlp/

# Pre-download embedding model (all-MiniLM-L6-v2) during build
# This reduces cold start time in production
RUN python3 -c "from sentence_transformers import SentenceTransformer; \
    print('Downloading embedding model...'); \
    model = SentenceTransformer('all-MiniLM-L6-v2'); \
    print('Model downloaded successfully')" && \
    rm -rf /root/.cache/pip

# Create non-root user for security
RUN useradd -m -u 1000 appuser && \
    chown -R appuser:appuser /app
USER appuser

# Health check (FastAPI /health endpoint)
# Use simpler health check to reduce overhead
HEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \
    CMD python3 -c "import http.client; \
        conn = http.client.HTTPConnection('localhost', 8080); \
        conn.request('GET', '/health'); \
        resp = conn.getresponse(); \
        exit(0 if resp.status == 200 else 1)"

# Expose port
EXPOSE 8080

# Run FastAPI with Uvicorn
# - 0.0.0.0:8080 for Cloud Run compatibility
# - 2 workers for better concurrency
# - --access-log for request logging
CMD ["uvicorn", "genai.api:app", \
     "--host", "0.0.0.0", \
     "--port", "8080", \
     "--workers", "2", \
     "--log-level", "info", \
     "--access-log"]
